from typing import Set, Iterable, List, Dict, Any, Optional, Union, Callable, Tuple
from dataclasses import dataclass
from datetime import datetime
from queue import Empty
from select import select
from enum import Enum
import multiprocessing
import multiprocessing.connection
import multiprocessing.context
import multiprocessing.process
import logging
import os

from .process import TaskName, Task, TaskMetrics, Process, UpstreamTasks, Resource, Resources


logging.basicConfig(level=os.environ.get('LOGLEVEL', 'INFO').upper())


WORKER_JOIN_TIMEOUT = 1


class FriendlyEnum(Enum):
    """ Enum which returns value name when coerced to string """

    def __str__(self):
        return self.name


class BroadcastScope(FriendlyEnum):
    """ Scope of a broadcast """

    TASK = 'Task'
    PROCESS = 'Process'


class BroadcastMode(FriendlyEnum):
    """ Mode of broadcast """

    REPLACE = 'Replace'
    APPEND = 'Append'


BroadcastKey = str
Broadcasts = Dict[Tuple[Optional[TaskName], BroadcastKey], Any]

class TaskStatus(FriendlyEnum):
    """ State of a task in an executing process """

    BLOCKED = 'Blocked'
    RUNNABLE = 'Runnable'
    RUNNING = 'Running'
    COMPLETED = 'Completed'


TaskRef = Union[Task, TaskName]


@dataclass
class TaskState():
    """ Information about a task state within an executing process """

    # Reference to task object itself
    task: Task
    # Metrics for task run-time
    metrics: TaskMetrics
    # Execution status of task
    status: TaskStatus = TaskStatus.BLOCKED
    # Subprocess
    worker: Optional[multiprocessing.process.BaseProcess] = None
    # Pipe for messages from subprocess
    pipe: Optional[multiprocessing.connection.Connection] = None

    def __hash__(self):
        return hash(self.task.name)


@dataclass
class ExecutionContext():
    """ Context passed to Task::execute method """

    """
    Subclassing Process will allow you to provide "global" immutable values,
    while the values returned by upstream tasks provide you with "parameters"
    generated by previous operations.

    Since each task is executed in a subprocess, changes made to variables in
    one task will not affect the values seen by other tasks (regardless of
    whether executing concurrently / sequentially).

    The only way for tasks to communicate information are for:
     - upstream tasks to return values, which their immediate downstreams will
       then be provided as parameters.
     - tasks to broadcast key-value pairs, which can then be read by downstream
       tasks.

    Note that broadcasts are not strictly limited to upstream/downstream.
    Depending on the order in which tasks start, it may be possible for a task
    to receive broadcasts from its siblings.  Therefore, PROCESS-scoped
    broadcasts should be used with caution.
    """

    # The process that the task is running as part of
    process: Process
    # The immediate upstream tasks that this one depends on
    upstreams: UpstreamTasks
    # Data broadcasted at the time that this task started
    broadcasts: Broadcasts
    # Call to broadcast data to tasks via the orchestrator
    broadcast: Callable[[BroadcastScope, BroadcastMode, BroadcastKey, Any], None]


class UpdateResult(FriendlyEnum):
    """ Result of orchestrator update """

    COMPLETED = 'Completed'
    FAILED = 'Failed'
    RUNNING = 'Running'


class WorkerMessage(FriendlyEnum):
    """ Messages from workers to master """

    FINALISE = 'Finalise'
    BROADCAST = 'Broadcast'


class Orchestrator():
    """ Executes a process """

    # Process being orchestrated
    process: Process
    # Approximate order to execute tasks in (topologically-sorted graph)
    task_state_list: List[TaskState]
    # Metrics for task runs
    id_map: Dict[TaskName, TaskState]
    # Broadcasted values
    broadcasts: Broadcasts
    # Resources remaining (process resources minus resources used by running tasks)
    resources: Resources
    # Multiprocessing context
    mp_context: multiprocessing.context.BaseContext
    # Tasks in each status
    state_map: Dict[TaskStatus, Set[TaskState]]

    def __init__(self, process: Process):
        logging.info('Orchestrating process %s which contains %d tasks', process.name, len(process.graph.nodes))
        self.process = process
        self.task_state_list = [
            TaskState(task=node.task, metrics=TaskMetrics(task_name=node.task.name))
            for node in
            self.process.graph.topological_sort()
        ]
        self.id_map = {
            state.task.name: state
            for state in self.task_state_list
        }
        self.broadcasts = dict()
        self.resources = process.resources.copy()
        self.mp_context = multiprocessing.get_context('fork')
        self.state_map = {
            TaskStatus.BLOCKED: set(self.task_state_list),
            TaskStatus.RUNNABLE: set(),
            TaskStatus.RUNNING: set(),
            TaskStatus.COMPLETED: set(),
        }
        # Check for tasks with the same name
        if len(self.id_map) < len(self.task_state_list):
            logging.error('Multiple tasks with the same name in the same process')
            raise KeyError()
        # Check for tasks that require more resources than the process has available
        for state in self.task_state_list:
            task = state.task
            for resource, quota in task.resources.items():
                if resource not in self.process.resources:
                    logging.error('Resource <%s> (quota %s) is required for task <%s> but is not provided by process <%s>', resource.name, quota, task.name, process.name)
                    raise KeyError()
                if quota > self.process.resources[resource]:
                    logging.error('Resource <%s> quota of %s for task <%s> exceeds maximum allocation %s for process <%s>', resource.name, quota, task.name, self.process.resources[resource], process.name)
                    raise ValueError()
        # Mark root tasks as "runnable"
        for state in self.task_state_list:
            task = state.task
            if not self.process.graph.incoming_edges[task.node]:
                self._set_task_status(state, TaskStatus.RUNNABLE)

    @staticmethod
    def _is_status_transition_valid(pre, post) -> bool:
        valid_transitions: Dict[TaskStatus, Set[TaskStatus]] = {
            TaskStatus.BLOCKED: { TaskStatus.RUNNABLE },
            TaskStatus.RUNNABLE: { TaskStatus.RUNNING },
            TaskStatus.RUNNING: { TaskStatus.COMPLETED },
            TaskStatus.COMPLETED: set(),
        }
        return post in valid_transitions[pre]

    def _set_task_status(self, state: TaskState, status: TaskStatus, force: bool = False) -> None:
        task = state.task
        if not self._is_status_transition_valid(state.status, status):
            if force:
                logging.warning('Forcing task to perform invalid status-transition: %s -> %s', state.status, status)
            else:
                logging.error('Invalid status-transition for %s: %s -> %s', task.name, state.status, status)
                raise ValueError()
        logging.debug('Task %s transitioned from status %s to %s', task.name, state.status, status)
        if state.status == status:
            return
        # Invariants check
        if not self._check_state_invariants(state, status):
            logging.error('State transition for %s of %s -> %s would violate invariant', task.name, state.status, status)
            raise ValueError()
        # Pre
        self.state_map[state.status].remove(state)
        # Set
        state.status = status
        # Post
        self.state_map[state.status].add(state)

    def _end_worker(self, state: TaskState) -> Optional[int]:
        if not state.worker:
            return None
        state.worker.join(WORKER_JOIN_TIMEOUT)
        if state.worker.is_alive():
            state.worker.kill()
        while state.pipe and state.pipe.poll(0):
            try:
                self._handle_worker_message(state, state.pipe.recv())
            except (EOFError, IOError):
                break
        if state.pipe:
            state.pipe.close()
            state.pipe = None
        state.worker.join()
        exitcode = state.worker.exitcode
        state.worker.close()
        state.worker = None
        metrics = state.metrics
        metrics.end_time = datetime.utcnow()
        metrics.duration = metrics.end_time - metrics.start_time # pyright: reportGeneralTypeIssues=false
        logging.info('Task %s worker exited with code %d after %.0f seconds', state.task.name, exitcode, metrics.duration.total_seconds())
        task = state.task
        logging.debug('Task %s releasing resources %s', task.name, task.resources)
        self._release_resource(task.resources)
        return exitcode

    @staticmethod
    def _check_state_invariants(state: TaskState, target_state: TaskStatus) -> bool:
        if target_state == TaskStatus.RUNNING:
            if any(item is None for item in (state.worker, state.pipe)):
                logging.error('Invariant violated: worker/pipe may only be set for RUNNING tasks')
                return False
        else:
            if any(item is not None for item in (state.worker, state.pipe)):
                logging.error('Invariant violated: worker/pipe may only be set for RUNNING tasks')
                return False
        if target_state == TaskStatus.COMPLETED:
            if not isinstance(state.metrics.success, bool):
                logging.error('Invariant violated: success/fail must be set for COMPLTED task')
                return False
        return True

    def _task_started(self, state: TaskState) -> None:
        self._set_task_status(state, TaskStatus.RUNNING)

    def _task_completed(self, state: TaskState) -> None:
        self._set_task_status(state, TaskStatus.COMPLETED)
        task = state.task
        # Queue up any downstream tasks that were only blocked by this one
        for node in self.process.graph.outgoing_edges[task.node]:
            next_state = self.id_map[node.task.name]
            if next_state.status == TaskStatus.BLOCKED:
                if all(
                        self.id_map[blocker.task.name].status == TaskStatus.COMPLETED
                        for blocker in self.process.graph.incoming_edges[node]
                ):
                    self._set_task_status(next_state, TaskStatus.RUNNABLE)

    def _acquire_resource(self, resources: Resources):
        for resource, quota in resources.items():
            self.resources[resource] -= quota
            if self.resources[resource] < 0:
                raise ValueError('Resource has been over-consumed, remaining value is negative')

    def _release_resource(self, resources: Resources):
        for resource, quota in resources.items():
            self.resources[resource] += quota

    def _get_blocking_resource_requirements(self, resources: Resources) -> Iterable[Resource]:
        return {
            resource
            for resource, quota in resources.items()
            if self.resources[resource] < quota
        }

    def _get_runnable_tasks(self) -> Set[TaskState]:
        return {
            state
            for state in map(lambda node: self.id_map[node.task.name], self.process.graph.nodes)
            if state.status == TaskStatus.RUNNABLE
            if not self.get_blockage_reason(state)
        }

    @staticmethod
    def _send_message_from_worker(pipe: multiprocessing.connection.Connection, message: WorkerMessage, *args) -> None:
        pipe.send((message, *args))

    @staticmethod
    def _task_subprocess_entry_point(task: Task, pipe: multiprocessing.connection.Connection, context: ExecutionContext) -> None:
        task_name = task.name
        logging.debug('Task %s worker starting', task_name)
        try:
            result = task.execute(context)
            success = True
            logging.debug('Task %s worker completed', task.name)
        except Exception as exc: # pylint: disable=broad-except
            logging.warning('Task %s worker failed with error %s', task.name, exc)
            result = exc
            success = False
        logging.debug('Task %s worker submitting result to master', task_name)
        Orchestrator._send_message_from_worker(pipe, WorkerMessage.FINALISE, success, result)
        logging.debug('Task %s worker stopping', task_name)
        pipe.close()

    def _start_task(self, state: TaskState):
        task = state.task
        metrics = state.metrics
        logging.debug('Task %s acquiring resources %s', task.name, task.resources)
        self._acquire_resource(task.resources)
        try:
            upstreams = {
                state.task.name: state.metrics
                for state in [
                    self.id_map[node.task.name]
                    for node in self.process.graph.incoming_edges[task.node]
                ]
            }
            def broadcast_from_worker(scope: BroadcastScope, mode: BroadcastMode, key: BroadcastKey, value: Any):
                self._send_message_from_worker(pipe_sink, WorkerMessage.BROADCAST, scope, mode, key, value)
            context = ExecutionContext(
                process=self.process,
                upstreams=upstreams,
                broadcasts=self.broadcasts,
                broadcast=broadcast_from_worker,
            )
            pipe_source, pipe_sink = self.mp_context.Pipe(False)
            worker = self.mp_context.Process(target=self._task_subprocess_entry_point, args=(task, pipe_sink, context))
            state.worker = worker
            state.pipe = pipe_source
            worker.name = f'Task: {task.name}'
            worker.start()
            metrics.start_time = datetime.utcnow()
            self._task_started(state)
            logging.info('Task %s worker spawned with pid %d', task.name, worker.pid)
        except Exception as exc: # pylint: disable=broad-except
            metrics.result = exc
            metrics.success = False
            self._end_worker(state)
            self._task_completed(state)
            logging.warning('Task %s failed to start: %s', task.name, exc)

    def _handle_worker_message(self, state: TaskState, fields) -> None:
        message = fields[0]
        args = fields[1:]
        if message == WorkerMessage.FINALISE:
            self._finalise_task(state, *args)
        elif message == WorkerMessage.BROADCAST:
            self._broadcast(state, *args)
        else:
            logging.warning('Received unknown message of type %s from worker for task %s', message, state.task.name)

    def _broadcast(self, state: TaskState, scope: BroadcastScope, mode: BroadcastMode, key: BroadcastKey, value: Any):
        logging.debug('Task %s broadcasted key %s with %s scope and value of type %s in mode %s', state.task.name, key, scope, type(value), mode)
        if scope == BroadcastScope.PROCESS:
            key_tuple = (None, key)
        elif scope == BroadcastScope.TASK:
            key_tuple = (state.task.name, key)
        else:
            logging.error('Invalid broadcast scope: %s', scope)
            raise ValueError()
        if mode == BroadcastMode.APPEND:
            if key_tuple not in self.broadcasts:
                self.broadcasts[key_tuple] = []
            self.broadcasts[key_tuple].append(value)
        elif mode == BroadcastMode.REPLACE:
            self.broadcasts[key_tuple].append(value)
        else:
            logging.error('Invalid broadcast mode: %s', mode)
            raise ValueError()

    def _finalise_task(self, state: TaskState, success: bool, result: Any) -> None:
        logging.debug('Task %s finalising', state.task.name)
        exitcode = self._end_worker(state)
        if exitcode:
            success = False
            result = Exception('Worker exited with non-zero exit-code', exitcode, result)
        metrics = state.metrics
        metrics.result = result
        metrics.success = success
        self._task_completed(state)
        logging.info('Task %s %s', state.task.name, 'completed' if success else 'failed')

    def _get_task_state_by_ref(self, ref: TaskRef) -> TaskState:
        if isinstance(ref, TaskState):
            return ref
        elif isinstance(ref, Task):
            return self.id_map[ref.name]
        elif isinstance(ref, TaskName):
            return self.id_map[ref]
        else:
            logging.error('Unsupported type: %s', type(ref))
            raise TypeError()

    def _get_task_state_by_refs(self, refs: Iterable[TaskRef]) -> List[TaskState]:
        return [self._get_task_state_by_ref(ref) for ref in refs]

    def _get_downstream_tasks(self, state: TaskState) -> Iterable[TaskState]:
        return [
            self.id_map[node.task.name]
            for node in self.process.graph.outgoing_edges[state.task.node]
        ]

    def _get_upstream_tasks(self, state: TaskState) -> Iterable[TaskState]:
        return [
            self.id_map[node.task.name]
            for node in self.process.graph.incoming_edges[state.task.node]
        ]

    @staticmethod
    def _get_tasks_recursively(roots: Iterable[TaskState], expander: Callable[[TaskState], Iterable[TaskState]]) -> Set[TaskState]:
        result = set(roots)
        previous = None
        current = set(roots)
        while current:
            previous = current
            current = set()
            for item in previous:
                current.update(expander(item))
            result.update(current)
        return result

    def get_blockage_reason(self, state: TaskState) -> Optional[str]:
        task = state.task
        if state.status == TaskStatus.BLOCKED:
            unmet_dependencies = sorted([
                node.task.name
                for node in self.process.graph.incoming_edges[state.task.node]
                if self.id_map[node.task.name].status != TaskStatus.COMPLETED
            ])
            return f'Task is waiting a dependencies to complete: {", ".join(unmet_dependencies)}'
        if state.status != TaskStatus.RUNNABLE:
            return f'Task is in {state.status} state'
        if blocking_resources := self._get_blocking_resource_requirements(task.resources):
            return f'Task is blocked by contention for resources: {", ".join([resource.name for resource in blocking_resources])}'
        return None

    def update(self, timeout: Optional[float] = None) -> UpdateResult:
        # Start any tasks that are runnable
        runnable_tasks = self._get_runnable_tasks()
        for state in runnable_tasks:
            self._start_task(state)
        if not (runnable_tasks or self.state_map[TaskStatus.RUNNING]):
            return UpdateResult.FAILED
        # Finalise any tasks that have ended
        try:
            class PollWrapper:
                """ Wrapper to provide fileno() and letting us recover state """
                def __init__(self, state: TaskState):
                    self.state = state
                def fileno(self) -> int:
                    return self.state.pipe.fileno()

            ignore = set()
            def get_workers_with_pending_messages(timeout: float, ignore: Set[TaskState]):
                rlist, _, _ = select([
                    PollWrapper(state)
                    for state in self.state_map[TaskStatus.RUNNING].difference(ignore)
                ], [], [], timeout or 0)
                return [wrapper.state for wrapper in rlist]
            timeout = timeout or 0
            while readable := get_workers_with_pending_messages(timeout, ignore):
                timeout = 0
                for state in readable:
                    try:
                        self._handle_worker_message(state, state.pipe.recv())
                    except EOFError:
                        ignore.add(state)
        except Empty:
            pass
        if all(state.status == TaskStatus.COMPLETED for state in self.task_state_list):
            return UpdateResult.COMPLETED
        return UpdateResult.RUNNING

    def run(self, poll_timeout: Optional[float] = None, idle_callback: Optional[Callable[[], None]] = None) -> bool:
        while True:
            result = self.update(timeout=poll_timeout)
            if result == UpdateResult.FAILED:
                return False
            elif result == UpdateResult.COMPLETED:
                return True
            elif idle_callback:
                idle_callback()

    def broadcast(self, ref: TaskRef, scope: BroadcastScope, mode: BroadcastMode, key: BroadcastKey, value: Any):
        state = self._get_task_state_by_ref(ref)
        self._broadcast(state, scope, mode, key, value)

    def get_task_status(self, ref: TaskRef) -> TaskStatus:
        return self._get_task_state_by_ref(ref).status

    def get_task_metrics(self, ref: TaskRef) -> TaskMetrics:
        return self._get_task_state_by_ref(ref).metrics

    def find_tasks(self, refs: Iterable[TaskRef], upstream: bool = False, downstream: bool = False, recursive: bool = False, statuses: Optional[Set[TaskStatus]] = None, exclude_original: bool = False) -> Iterable[TaskName]:
        roots = set(self._get_task_state_by_refs(refs))
        result = set()
        if upstream:
            expander = self._get_upstream_tasks
            if recursive:
                result.update(self._get_tasks_recursively(roots, expander))
            else:
                for root in roots:
                    result.update(expander(root))
        if downstream:
            expander = self._get_downstream_tasks
            if recursive:
                result.update(self._get_tasks_recursively(roots, expander))
            else:
                for root in roots:
                    result.update(expander(root))
        if exclude_original:
            result.difference_update(roots)
        return [
            state.task.name
            for state in result
            if statuses is None or state.status in statuses
        ]

    def reset_tasks(self, refs: Iterable[TaskRef], status: TaskStatus = TaskStatus.BLOCKED):
        states = set(self._get_task_state_by_refs(refs))
        for state in states:
            if state.status == status:
                logging.warning('Forced status change of %s -> %s for task %s, skipping', status, status, state.task.name)
                continue
            if state.status == TaskStatus.RUNNING:
                logging.warning('Forced status change from Running for task %s, terminating worker', state.task.name)
                state.worker.terminate()
                self._end_worker(state)
            if status == TaskStatus.COMPLETED:
                logging.warning('Forced status change to Completed for task %s, setting success=False', state.task.name)
                state.metrics.success = False
            if state.status == TaskStatus.COMPLETED:
                old_metrics = state.metrics
                state.metrics = TaskMetrics(task_name=old_metrics.task_name, previous=old_metrics)
            logging.warning('Forced status change for task %s: %s -> %s', state.task.name, state.status, status)
            self._set_task_status(state, status, force=True)
